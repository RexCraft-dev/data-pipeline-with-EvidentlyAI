{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install evidently"
      ],
      "metadata": {
        "id": "2XnKuWOIyn6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHExfGKi9Y5q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "\n",
        "data = yf.download(\"GOOG\", start=\"2011-01-01\", end=\"2024-12-31\").reset_index()"
      ],
      "metadata": {
        "id": "BwYaXQzGXvMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert MultiIndex column names to a single index by removing second level\n",
        "if isinstance(data.columns, pd.MultiIndex):\n",
        "    data.columns = data.columns.droplevel('Ticker')\n",
        "data.columns.name = None\n",
        "\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "BYEaGUiIb8cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# group and split into batches (yearly)\n",
        "grouped = data.groupby(data['Date'].dt.year)\n",
        "batch_data = [group for _, group in grouped]\n",
        "\n",
        "for batch in batch_data:\n",
        "  # Calculate VWAP (Volume Weighted Average Price)\n",
        "  batch['vwap'] = (batch['Close'] * batch['Volume']).cumsum() / batch['Volume'].cumsum()\n",
        "  batch.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # Select a random column\n",
        "  random_column = random.choice([col for col in batch.columns if col != 'vwap' and col != 'Date'])\n",
        "\n",
        "  # Determine how many values to nullify (between 0 and 50)\n",
        "  num_nulls = random.randint(0, 5)\n",
        "\n",
        "  # Select random indices to replace with NaN\n",
        "  null_indices = random.sample(range(len(batch)), num_nulls) if num_nulls > 0 else []\n",
        "\n",
        "  # Assign NaN to selected indices\n",
        "  batch.loc[null_indices, random_column] = None"
      ],
      "metadata": {
        "id": "N3ylvjc2N2hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evidently.metric_preset import DataQualityPreset\n",
        "from evidently.report import Report\n",
        "\n",
        "df = pd.concat(batch_data, axis=0).reset_index(drop=True)\n",
        "\n",
        "data_quality_report = Report(metrics=[\n",
        "    DataQualityPreset(),\n",
        "])\n",
        "\n",
        "data_quality_report.run(reference_data=None, current_data=df)\n",
        "data_quality_report.show(mode='inline')"
      ],
      "metadata": {
        "id": "oCyWxX78pfMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Provided preprocessing function\n",
        "def preprocessing(df):\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    cols_to_normalize = ['Open', 'High', 'Low', 'Close', 'Volume', 'vwap']\n",
        "    scaler = StandardScaler()\n",
        "    df[cols_to_normalize] = scaler.fit_transform(df[cols_to_normalize])\n",
        "\n",
        "    df['target'] = (df['Close'].diff() > 0).astype(int)\n",
        "    df.loc[0, 'target'] = 0\n",
        "\n",
        "    X = df.drop(columns=['Date', 'target'])\n",
        "    y = df['target'].astype(int)\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "OxThLmEZcRba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from evidently.metric_preset import DataDriftPreset, DataQualityPreset, TargetDriftPreset\n",
        "from evidently.report import Report\n",
        "\n",
        "# Initialize lists to store results\n",
        "drift_reports = []\n",
        "quality_reports = []\n",
        "target_drift_reports = []\n",
        "\n",
        "# Iterate through yearly batches (starting from year index 3 to ensure reference data exists)\n",
        "for i in range(3, len(batch_data)):\n",
        "    print(f\"Processing Year: {batch_data[i]['Date'].dt.year.iloc[0]}...\")\n",
        "\n",
        "    # Define reference and current datasets\n",
        "    reference_data = pd.concat(batch_data[i-3:i-1]).reset_index(drop=True)  # Previous 2 years as reference\n",
        "    current_data = batch_data[i].reset_index(drop=True)  # Current year as test set\n",
        "\n",
        "    # Preprocess reference and current data\n",
        "    ref_X, ref_y = preprocessing(reference_data)\n",
        "    curr_X, curr_y = preprocessing(current_data)\n",
        "\n",
        "    # Train logistic regression model\n",
        "    model = LogisticRegression()\n",
        "    model.fit(ref_X, ref_y)\n",
        "\n",
        "    # Predict on current batch\n",
        "    y_pred = model.predict(curr_X)\n",
        "    accuracy = accuracy_score(curr_y, y_pred)\n",
        "\n",
        "    print(f\"Accuracy for Year {batch_data[i]['Date'].dt.year.iloc[0]}: {accuracy:.4f}\")\n",
        "\n",
        "    # Generate Evidently AI reports\n",
        "    quality_report = Report(metrics=[DataQualityPreset()])\n",
        "    quality_report.run(reference_data=None, current_data=current_data)\n",
        "    quality_reports.append(quality_report)\n",
        "\n",
        "    drift_report = Report(metrics=[DataDriftPreset()])\n",
        "    drift_report.run(reference_data=reference_data, current_data=current_data)\n",
        "    drift_reports.append(drift_report)\n",
        "\n",
        "    target_drift_report = Report(metrics=[TargetDriftPreset()])\n",
        "    target_drift_report.run(reference_data=reference_data, current_data=current_data)\n",
        "    target_drift_reports.append(target_drift_report)"
      ],
      "metadata": {
        "id": "usfzvIO_Bdhb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}